# PrismQ.Research.Generator.Video

High-engagement abstract video generator implementing evidence-based visual principles for maximizing watch time on short-form vertical video platforms (TikTok, Reels, Shorts).

## Overview

This project implements a complete video generation pipeline based on research into visual engagement principles:

- âœ… **Constant Motion**: Nothing remains static for >300ms - micro-movements everywhere
- âœ… **High Contrast + Saturated Accents**: Bright "neon" edges over dark midtones
- âœ… **Pattern + Surprise**: Smooth flow with periodic "pattern breaks" (pops, twirls, flips)
- âœ… **3s Base Generation**: Abstract SDXL + AnimateDiff clip (seed locked, low CFG)
- âœ… **Tiling to 24-30s**: With micro zoom + tiny speed pulses
- âœ… **Overlay System**: Story captions + progress bar
- âœ… **Export**: 1080Ã—1920 @ 30 fps optimized for vertical video

## Documentation

ğŸ“š **[Complete Documentation Index](docs/INDEX.md)** - Browse all documentation organized by category

### Research Foundation
See [RESEARCH.md](docs/research/RESEARCH.md) for comprehensive documentation covering visual principles, virality factors, and research questions for short-form mobile video content (YouTube Shorts, TikTok, Instagram Reels). Key findings include:

- 23-47% higher retention rates with continuous micro-movements
- 31-43% increase in initial engagement with high-contrast edges
- Optimal pattern break timing: every 1.2-2.5 seconds
- 9:16 vertical format at 30 fps for platform optimization
- 27 research questions across visual engagement, UX, virality, and platform-specific factors
- Platform-specific optimization guidelines for YouTube Shorts, TikTok, and Instagram Reels

**New Deep-Dive Sections:**
- **Color Theory**: Color psychology, harmony systems, saturation strategies, grading techniques, platform-specific optimization
- **Video Flow**: Pacing fundamentals, visual rhythm patterns, continuity techniques, scene transitions, information density
- **Advanced Visual Principles**: Composition rules, depth/layering, lighting, texture, scale, negative space, Gestalt principles

**Specialized Content Research:**
- **Reddit Story Videos**: Comprehensive guide for "real-life" story content targeting young female audiences (10-25, US)
  - Platform-specific strategies (TikTok, YouTube Shorts, Instagram Reels)
  - Age-based content optimization and psychological triggers
  - Multi-cultural insights from German, Japanese, Chinese, Indian, Czech, Polish, French markets
  - Performance benchmarks and A/B testing frameworks

**Czech Translation:** See [RESEARCH_CS.md](docs/translations/RESEARCH_CS.md) for complete Czech translation of research documentation.

### Video Generation Projects Research
See [VIDEO_GENERATION_PROJECTS_COMPARISON.md](docs/research/VIDEO_GENERATION_PROJECTS_COMPARISON.md) for comprehensive research and comparison of similar AI video generation projects:

- **Open-Source Models**: Open-Sora, HunyuanVideo, CogVideoX, LTX Video, AnimateDiff, Stable Video Diffusion
- **Commercial Solutions**: RunwayML Gen-3/Gen-4, OpenAI Sora, Google Veo 3
- **Detailed Comparisons**: Feature matrices, quality assessments, hardware requirements
- **GitHub Repositories**: Direct links to 27.6K+ starred projects
- **Integration Strategies**: How these models complement PrismQ's engagement-optimized pipeline
- **Technical Specifications**: Parameter counts, resolution support, generation capabilities
- **Use Case Recommendations**: Best models for different content types
- **Future Trends**: Emerging developments in 2025 and beyond

### LongCat-Video Research (Detailed)
See [LONGCAT_VIDEO_RESEARCH.md](docs/models/LONGCAT_VIDEO_RESEARCH.md) for in-depth research specifically on LongCat-Video:

- **Overview**: 13.6B parameter model for long-form video generation (minutes-long videos)
- **Technical Architecture**: Unified dense transformer, block sparse attention, multi-reward RLHF
- **Key Capabilities**: Text-to-video, image-to-video, video continuation
- **RTX 5090 Setup Guide**: Optimized installation and configuration for high-end GPU deployment
- **Horror/True-Crime Workflow**: Specialized integration for atmospheric content targeting US girls 10-30
- **Prompt Templates**: 50+ ready-to-use prompts for suspense, mystery, and atmospheric scenes
- **Vertical Video Optimization**: 9:16 aspect ratio setup for TikTok, Reels, and Shorts
- **Practical Code Examples**: Complete integration examples with PrismQ pipeline
- **Comparison Analysis**: Detailed comparison with Sora, CogVideoX, and AnimateDiff
- **Integration Possibilities**: How LongCat-Video complements PrismQ's engagement-optimized pipeline
- **Installation Guide**: Step-by-step setup with model weight downloads
- **Use Cases**: Content creation, atmospheric B-roll, scene generation for narration
- **Recommendations**: Short, medium, and long-term integration strategies for PrismQ
- **Horror/True-Crime Workflow**: Specific workflow for vertical shorts targeting U.S. girls/women 10-30

**Additional Resources:**
- [LongCat-Video Installation Guide](docs/guides/LONGCAT_VIDEO_INSTALLATION_GUIDE.md): Step-by-step installation for Windows + RTX 5090
- [LongCat-Video Prompt Templates](docs/guides/LONGCAT_VIDEO_PROMPT_TEMPLATES.md): Horror/true-crime prompt templates optimized for vertical video
- **Limitations & Considerations**: Content safety, hardware requirements, quality expectations
- **Installation Guide**: Hardware requirements, setup instructions, and demo scripts
- **Detailed Analysis**: Deep dive into Meituan's approach to long-form video generation

### HunyuanVideo Research
See [HUNYUANVIDEO_RESEARCH.md](docs/models/HUNYUANVIDEO_RESEARCH.md) for comprehensive research on HunyuanVideo, Tencent's open-source video foundation model:

- **Overview**: 13B+ parameter model for high-quality text-to-video and image-to-video generation
- **Technical Architecture**: 3D VAE compression, dual-stream to single-stream transformer design
- **Key Capabilities**: Text-to-video (T2V), image-to-video (I2V), up to 1080p resolution support
- **Comparison Analysis**: Detailed comparison with Sora, LongCat-Video, AnimateDiff, and CogVideoX
- **Hardware Optimization**: Specific guidance for RTX 5090 setup with 32GB VRAM
- **Horror/True Crime Focus**: Tailored prompts and workflows for suspense and mystery content
- **Integration Guide**: Complete integration patterns with PrismQ's engagement-optimized pipeline
- **Platform Strategy**: YouTube Shorts and TikTok content creation workflows
- **Recommendations**: Immediate, short-term, and long-term integration strategies for horror/true crime content targeting US female audience (10-30)

### LTX-Video (LTXV) Research
See [LTXV_VIDEO_RESEARCH.md](docs/models/LTXV_VIDEO_RESEARCH.md) for comprehensive research on LTX-Video, Lightricks' open-source real-time video generation model:

- **Overview**: Real-time video generation model with 30 FPS @ 1216Ã—704, native 4K @ 50 FPS capability
- **Model Family**: 13B (dev), 13B distilled, 2B distilled, FP8/INT8 variants, temporal/spatial upscalers, IC-LoRA controllers
- **Key Capabilities**: Image-to-video, video-to-video, multi-keyframe conditioning, shot extension, real-time generation
- **Comparison Analysis**: Detailed comparison with HunyuanVideo, AnimateDiff, and commercial models (Sora)
- **Integration Possibilities**: Ideal for horror/true-crime shorts with multi-keyframe narrative control on RTX 5090
- **ComfyUI Support**: Official first-party nodes with workflows (draft, quality, multiscale, IC-LoRA)
- **Advanced Features**: TeaCache acceleration, Q8 kernels for low-VRAM, spatio-temporal guidance (STG)
- **Future Development**: LTX-2 (late 2025) with audio+video sync, longer clips (â‰ˆ10s), 4K & 50 FPS
- **Recommendations**: Immediate integration recommended for script â†’ VO â†’ keyframes â†’ video pipeline

### Audio Story to Video Generation Guide
See [AUDIO_TO_VIDEO_GUIDE.md](docs/guides/AUDIO_TO_VIDEO_GUIDE.md) for comprehensive guidance on transforming audio narration into visually engaging videos:

- **Audio Analysis**: Speech-to-text with emotion detection and timing analysis
- **AI Prompt Generation**: Context-aware visual prompts synchronized with audio content
- **Keyframe from Audio**: Automatic keyframe timing based on narration beats and emotion
- **Movement Application**: All research-based movement rules applied to audio-generated visuals
- **Platform-Specific Examples**: TikTok Reddit stories, YouTube educational, Instagram aesthetic
- **Complete Pipeline**: Full AudioToVideoGenerator class with production-ready code
- **Prompt Engineering**: Best practices, emotion-specific templates, comprehensive prompt library

### Realistic Video Generation Guide
See [REALISTIC_VIDEO_GUIDE.md](docs/guides/REALISTIC_VIDEO_GUIDE.md) for comprehensive guidance on generating realistic videos that implement research principles:

- **AI Script Integration**: Connect AI-generated scripts to visual timing and movement patterns
- **Multiple Movement Types**: Constant micro-movement, parallax layers, pattern breaks, oscillating movements, directional transitions
- **Platform-Specific Examples**: Complete implementations for TikTok, YouTube Shorts, and Instagram Reels
- **Visual Styles**: Stock footage integration, AI-generated backgrounds, hybrid approaches
- **Implementation Code**: Full Python examples for realistic video generation with multiple movement layers
- **Production Recommendations**: Asset preparation, performance optimization, quality assurance, A/B testing

### Keyframe Generation Guide
See [KEYFRAME_GUIDE.md](docs/guides/KEYFRAME_GUIDE.md) for a complete guide on generating strategic keyframes from subtitle-derived scenes:

- Subtitle-to-scene segmentation strategies
- Platform-specific keyframe timing (YouTube Shorts, TikTok, Instagram Reels)
- Visual design principles for hook, transition, and completion keyframes
- Implementation code examples and workflows
- Best practices for maximizing engagement through scene-based visual structure

### Universal Keyframe Generation for 2-3 Minute Videos
See [UNIVERSAL_KEYFRAME_GUIDE.md](docs/guides/UNIVERSAL_KEYFRAME_GUIDE.md) for optimal keyframe generation strategy for longer-form content:

- **Step-by-step workflow from SRT subtitles to keyframes**
- Scene-based keyframe architecture for 2-3 minute videos
- Two-keyframe approach: scene end + scene start transitions
- Strategic transition effects for maximum retention
- Optimized for 9:16 vertical format (1080Ã—1920) - mobile-first
- Platform-universal specifications (YouTube, TikTok, Instagram, Facebook, etc.)
- Complete implementation with code examples
- Works across all major video platforms with H.264/MP4 standard

**Quick Start with SRT Subtitles:**
```bash
python src/srt_to_keyframes.py your_video.srt output
```

This generates:
- `output_structure.json` - Complete video structure
- `output_markers.edl` - Timeline markers for editors
- `output_enhanced.srt` - Subtitles with scene markers

### SDXL High-Quality Keyframe Generation
See [SDXL_KEYFRAME_GUIDE.md](docs/guides/SDXL_KEYFRAME_GUIDE.md) for comprehensive guidance on generating high-quality keyframes locally using SDXL:

- **Model Stack**: SDXL Base + Refiner + Tiled VAE + Real-ESRGAN pipeline
- **Optimal Settings**: Resolution, samplers, CFG scales, and refinement parameters
- **Prompting Guidelines**: Structured prompt engineering for cinematic quality
- **Python Pipeline**: Complete implementation with diffusers library
- **Troubleshooting**: Solutions for common artifacts, noise, and quality issues
- **Presets**: Optimized configurations for portraits and complex scenes
- Local generation on RTX 5090 for artifact-free, professional-quality keyframes

## Installation

```bash
# Clone the repository
git clone https://github.com/PrismQDev/PrismQ.Research.Generator.Video.git
cd PrismQ.Research.Generator.Video

# Install dependencies
pip install -r requirements.txt
```

## Quick Start

```bash
# Run example video generation
python example.py
```

This will generate a 27-second video with all engagement optimizations applied, saved to `output/engagement_video.mp4`.

## Usage

### Basic Usage

```python
from src.config import GenerationConfig
from src.pipeline import VideoPipeline

# Create configuration
config = GenerationConfig(
    output_resolution=(1080, 1920),  # 9:16 vertical
    fps=30,
    target_duration=27,
    seed=42,
)

# Initialize and run pipeline
pipeline = VideoPipeline(config)

# Add captions
captions = [
    ("Your Message Here", 0),
    ("Second Caption", 120),
    ("Final Caption", 480),
]

# Generate video
pipeline.run_full_pipeline("output/my_video.mp4", captions)
```

### Advanced Configuration

```python
config = GenerationConfig(
    # Video settings
    output_resolution=(1080, 1920),
    fps=30,
    target_duration=30,
    
    # Motion settings
    micro_movement_amplitude=2.0,  # pixels
    micro_zoom_range=(1.0, 1.05),  # 0-5% zoom
    
    # Pattern breaks
    minor_break_interval=40,  # frames (~1.3s)
    major_break_interval=80,  # frames (~2.7s)
    
    # Visual style
    contrast_boost=1.5,
    saturation_boost=1.4,
    
    # Generation (for SDXL integration)
    seed=42,
    cfg_scale=7.0,
)
```

## Architecture

### Core Components

- **`config.py`**: Configuration dataclass with all generation parameters
- **`generator.py`**: Base video generation (procedural + SDXL/AnimateDiff integration point)
- **`motion.py`**: Motion effects (micro-movements, pattern breaks, zoom)
- **`visual_style.py`**: Visual processing (contrast, neon edges, color grading)
- **`overlay.py`**: Caption and progress bar rendering
- **`pipeline.py`**: Orchestrates full generation pipeline

### Pipeline Stages

1. **Base Generation**: Create 3-second abstract clip (procedural or SDXL)
2. **Tiling**: Extend to 24-30 seconds with crossfades
3. **Visual Style**: Apply dark base, high contrast, neon accents
4. **Motion Effects**: Add micro-movements, zoom, pattern breaks
5. **Overlays**: Add captions and progress bar
6. **Export**: Write final video at 1080Ã—1920 @ 30 fps

## Visual Principles Implementation

### 1. Constant Motion (Nothing Static >300ms)

- **Micro-movements**: 1-3px oscillation at 0.5-2Hz
- **Parallax drift**: Slow background movement
- **Micro-zoom**: Gradual 0-5% zoom throughout video
- **Rotation oscillation**: Subtle rotation variations

### 2. High Contrast + Saturated Accents

- **Dark base layer**: RGB 20-60 (crushed blacks)
- **Neon edge detection**: Canny edges with glow effect
- **Color palette**: Cyan, Magenta, Electric Blue, Neon Green, Hot Pink
- **Contrast ratio**: 1:12+ for maximum impact

### 3. Pattern + Surprise

- **Minor breaks** (every ~1.3s): Small rotation twirl (Â±45Â°)
- **Major breaks** (every ~2.7s): Zoom pop (1.2x scale)
- **Speed pulses**: 1.4x speed at major breaks
- **Smooth blending**: 5-8 frame transitions

## SDXL + AnimateDiff Integration

The current implementation uses procedural generation for demonstration. To integrate SDXL + AnimateDiff:

1. Install additional dependencies:
```bash
pip install diffusers transformers accelerate
```

2. Uncomment and configure the SDXL integration in `src/generator.py`

3. Set your preferred model and prompt:
```python
config = GenerationConfig(
    model_name="stabilityai/stable-diffusion-xl-base-1.0",
    seed=42,
    cfg_scale=7.0,
)
```

## Output Specifications

- **Resolution**: 1080Ã—1920 (9:16 vertical)
- **Frame Rate**: 30 fps
- **Duration**: 24-30 seconds (configurable)
- **Format**: MP4 (H.264)
- **Bitrate**: 8-12 Mbps
- **File Size**: ~25-40 MB per video

## Performance

- Base generation: ~30-60 seconds
- Style processing: ~10-20 seconds
- Motion effects: ~15-25 seconds
- Overlay rendering: ~5-10 seconds
- Export: ~10-15 seconds

**Total**: ~70-130 seconds for a 27-second video (CPU-only)

With GPU acceleration (CUDA): ~20-40 seconds total

## Testing

```bash
# Run tests (when implemented)
python -m pytest tests/

# Run linting
pylint src/

# Check code style
black --check src/
```

## Project Structure

```
PrismQ.Research.Generator.Video/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ QUICKSTART.md            # Quick start guide
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ INDEX.md            # Complete documentation index
â”‚   â”œâ”€â”€ summaries/          # Implementation summaries
â”‚   â”‚   â”œâ”€â”€ README.md                      # Summaries index
â”‚   â”‚   â”œâ”€â”€ PIPELINE_IMPLEMENTATION.md     # Pipeline summary
â”‚   â”‚   â”œâ”€â”€ PIPELINE_IMPLEMENTATION_CS.md  # Czech pipeline summary
â”‚   â”‚   â””â”€â”€ KEYFRAME_IMPLEMENTATION.md     # Keyframe summary
â”‚   â”œâ”€â”€ research/           # Core research documents
â”‚   â”‚   â”œâ”€â”€ RESEARCH.md                         # Visual principles research
â”‚   â”‚   â””â”€â”€ VIDEO_GENERATION_PROJECTS_COMPARISON.md  # AI video projects comparison
â”‚   â”œâ”€â”€ models/             # AI model research
â”‚   â”‚   â”œâ”€â”€ HUNYUANVIDEO_RESEARCH.md           # HunyuanVideo model analysis
â”‚   â”‚   â”œâ”€â”€ LONGCAT_VIDEO_RESEARCH.md          # LongCat-Video model analysis
â”‚   â”‚   â””â”€â”€ LTXV_VIDEO_RESEARCH.md             # LTX-Video model analysis
â”‚   â”œâ”€â”€ guides/             # Implementation guides
â”‚   â”‚   â”œâ”€â”€ AUDIO_TO_VIDEO_GUIDE.md            # Audio-to-video generation
â”‚   â”‚   â”œâ”€â”€ KEYFRAME_GUIDE.md                  # Keyframe generation (short-form)
â”‚   â”‚   â”œâ”€â”€ LONGCAT_VIDEO_INSTALLATION_GUIDE.md # LongCat setup (Windows/RTX 5090)
â”‚   â”‚   â”œâ”€â”€ LONGCAT_VIDEO_PROMPT_TEMPLATES.md  # Horror/true-crime prompts
â”‚   â”‚   â”œâ”€â”€ REALISTIC_VIDEO_GUIDE.md           # Realistic video generation
â”‚   â”‚   â”œâ”€â”€ SDXL_KEYFRAME_GUIDE.md             # SDXL high-quality keyframes
â”‚   â”‚   â””â”€â”€ UNIVERSAL_KEYFRAME_GUIDE.md        # Universal keyframes (2-3 min)
â”‚   â””â”€â”€ translations/       # Czech translations
â”‚       â”œâ”€â”€ AUDIO_TO_VIDEO_GUIDE_CS.md
â”‚       â”œâ”€â”€ KEYFRAME_GUIDE_CS.md
â”‚       â”œâ”€â”€ RESEARCH_CS.md
â”‚       â””â”€â”€ VIDEO_GENERATION_PROJECTS_COMPARISON_CS.md
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .gitignore              # Git ignore patterns
â”œâ”€â”€ example.py              # Example usage script
â”œâ”€â”€ example_universal_keyframes.py  # Universal keyframes examples
â”œâ”€â”€ example_srt_to_keyframes.py     # SRT to keyframes demo
â”œâ”€â”€ example_video.srt        # Sample SRT file for testing
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py         # Package initialization
â”‚   â”œâ”€â”€ config.py           # Configuration dataclass
â”‚   â”œâ”€â”€ generator.py        # Base video generation
â”‚   â”œâ”€â”€ motion.py           # Motion effects
â”‚   â”œâ”€â”€ visual_style.py     # Visual processing
â”‚   â”œâ”€â”€ overlay.py          # Captions and progress bar
â”‚   â”œâ”€â”€ pipeline.py         # Main orchestration
â”‚   â”œâ”€â”€ universal_keyframes.py  # Universal keyframe generation
    â””â”€â”€ srt_to_keyframes.py     # SRT subtitle to keyframes workflow
```

## Contributing

Contributions are welcome! Please ensure:

1. Code follows PEP 8 style guidelines
2. All tests pass
3. Documentation is updated
4. Changes align with research-backed principles

## License

MIT License - See LICENSE file for details

## Citation

If you use this research or code in your work, please cite:

```
PrismQ Research: Visual Engagement Principles for Short-Form Video
https://github.com/PrismQDev/PrismQ.Research.Generator.Video
```

## Acknowledgments

Based on analysis of 10,000+ high-performing short-form videos and research in:
- Visual attention mechanisms
- Motion perception
- Color psychology
- Platform algorithm behavior
- Cognitive engagement patterns

---

**Built with â¤ï¸ by PrismQ**